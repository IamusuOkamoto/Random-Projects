{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, time, itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d83680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-10 CNN met Keras — visuals & GIF voor LinkedIn\n",
    "# Vereist: tensorflow >= 2.10, scikit-learn, matplotlib, imageio\n",
    "\n",
    "import os, time, itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ========== 0) Reproduceerbaarheid & device ==========\n",
    "SEED = 42\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism = False  # zet True voor perfecte determinisme (kan trager zijn)\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"GPU available:\", len(tf.config.list_physical_devices('GPU')) > 0)\n",
    "\n",
    "# ========== 1) Data ==========\n",
    "(num_classes, input_shape) = (10, (32, 32, 3))\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "y_train = y_train.squeeze(); y_test = y_test.squeeze()\n",
    "\n",
    "mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "std  = np.array([0.2023, 0.1994, 0.2010])\n",
    "\n",
    "def normalize(x):\n",
    "    return (x/255.0 - mean) / std\n",
    "\n",
    "x_train = normalize(x_train.astype(\"float32\"))\n",
    "x_test  = normalize(x_test.astype(\"float32\"))\n",
    "\n",
    "batch_size = 128\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "augment = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomCrop(32, 32),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomContrast(0.1),\n",
    "        layers.RandomBrightness(factor=0.1),\n",
    "    ],\n",
    "    name=\"augment\",\n",
    ")\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
    "    .shuffle(10_000, seed=SEED) \\\n",
    "    .batch(batch_size) \\\n",
    "    .map(lambda x,y: (augment(x, training=True), y), num_parallel_calls=AUTOTUNE) \\\n",
    "    .prefetch(AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "# ========== 2) Model ==========\n",
    "def make_model():\n",
    "    inp = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(inp)\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.MaxPool2D()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\", name=\"last_conv\")(x)\n",
    "    x = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inp, out)\n",
    "    return model\n",
    "\n",
    "model = make_model()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=3e-4, weight_decay=1e-2),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# ========== 3) Train ==========\n",
    "EPOCHS = 20\n",
    "history = model.fit(train_ds, validation_data=test_ds, epochs=EPOCHS, verbose=1)\n",
    "\n",
    "# ========== 4) Evaluatie ==========\n",
    "test_loss, test_acc = model.evaluate(test_ds, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")\n",
    "\n",
    "# ========== 5) Helpers voor visualisaties ==========\n",
    "classes = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
    "\n",
    "def denorm(x):\n",
    "    # x in [-?,?] gestandaardiseerd; zet terug naar 0..1 voor plot\n",
    "    return np.clip(x*std + mean, 0, 1)\n",
    "\n",
    "def make_prediction_grid(x, y, probs, n=64, ncol=8, fp_only=False):\n",
    "    idx = np.arange(len(x))\n",
    "    if fp_only:\n",
    "        preds = probs.argmax(1)\n",
    "        wrong = idx[preds != y]\n",
    "        idx = wrong if len(wrong) > 0 else idx\n",
    "    idx = idx[:n]\n",
    "    xg = denorm(x[idx])\n",
    "    preds = probs[idx].argmax(1)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i, k in enumerate(idx):\n",
    "        plt.subplot(n//ncol, ncol, i+1)\n",
    "        plt.imshow(xg[i])\n",
    "        color = \"g\" if preds[i] == y[k] else \"r\"\n",
    "        plt.title(f\"P:{classes[preds[i]]}\\nT:{classes[y[k]]}\", fontsize=8, color=color)\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.arange(num_classes))\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.imshow(cm, cmap=\"Blues\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, classes, rotation=45, ha=\"right\")\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.title(\"CIFAR-10 Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    return cm\n",
    "\n",
    "# ========== 6) Voorspellingen voor visuals ==========\n",
    "y_prob = model.predict(test_ds, verbose=0)\n",
    "y_pred = y_prob.argmax(1)\n",
    "\n",
    "# ========== 7) Opslaan van artifacts ==========\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "# Confusion matrix\n",
    "_ = plot_confusion_matrix(y_test, y_pred)\n",
    "plt.savefig(\"artifacts/confusion_matrix.png\", dpi=200); plt.close()\n",
    "\n",
    "# Prediction grid (mix) & foute voorspellingen grid\n",
    "x_test_vis = x_test  # genormaliseerd; denorm doen we in plotter\n",
    "make_prediction_grid(x_test_vis, y_test, y_prob, n=64, ncol=8, fp_only=False)\n",
    "plt.savefig(\"artifacts/prediction_grid.png\", dpi=200); plt.close()\n",
    "\n",
    "make_prediction_grid(x_test_vis, y_test, y_prob, n=64, ncol=8, fp_only=True)\n",
    "plt.savefig(\"artifacts/wrong_predictions_grid.png\", dpi=200); plt.close()\n",
    "\n",
    "# ========== 8) Grad-CAM (op laatste conv-laag 'last_conv') ==========\n",
    "def gradcam(imgs, model, layer_name=\"last_conv\", class_idx=None):\n",
    "    # imgs = N x 32 x 32 x 3 (genormaliseerd)\n",
    "    grad_model = keras.Model([model.inputs], [model.get_layer(layer_name).output, model.output])\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_out, preds = grad_model(imgs, training=False)\n",
    "        if class_idx is None:\n",
    "            class_idx = tf.argmax(preds, axis=1)\n",
    "        one_hot = tf.one_hot(class_idx, num_classes)\n",
    "        loss = tf.reduce_sum(one_hot * preds)\n",
    "    grads = tape.gradient(loss, conv_out)          # [N,H,W,C]\n",
    "    weights = tf.reduce_mean(grads, axis=(1,2), keepdims=True)\n",
    "    cam = tf.nn.relu(tf.reduce_sum(weights * conv_out, axis=-1, keepdims=True))  # [N,H,W,1]\n",
    "    # normalize per image\n",
    "    cam_min = tf.reduce_min(cam, axis=(1,2,3), keepdims=True)\n",
    "    cam_max = tf.reduce_max(cam, axis=(1,2,3), keepdims=True)\n",
    "    cam = (cam - cam_min) / (cam_max - cam_min + 1e-6)\n",
    "    return cam.numpy()\n",
    "\n",
    "# Visualiseer Grad-CAM op 8 samples\n",
    "idx = np.random.RandomState(SEED).choice(len(x_test), size=8, replace=False)\n",
    "imgs = x_test[idx]\n",
    "cams = gradcam(imgs, model, \"last_conv\")\n",
    "vis = denorm(imgs)\n",
    "\n",
    "fig, axs = plt.subplots(2, 8, figsize=(16,4))\n",
    "for i in range(8):\n",
    "    axs[0,i].imshow(vis[i]); axs[0,i].axis(\"off\")\n",
    "    axs[1,i].imshow(vis[i]); axs[1,i].imshow(cams[i,:,:,0], alpha=0.5, cmap=\"jet\"); axs[1,i].axis(\"off\")\n",
    "axs[0,0].set_ylabel(\"Original\"); axs[1,0].set_ylabel(\"Grad-CAM\")\n",
    "plt.tight_layout(); plt.savefig(\"artifacts/gradcam_grid.png\", dpi=200); plt.close()\n",
    "\n",
    "# ========== 9) (Optioneel) GIF met wisselende voorspellingen ==========\n",
    "frames = []\n",
    "for _ in range(6):\n",
    "    sel = np.random.RandomState(None).choice(len(x_test), size=16, replace=False)\n",
    "    imgs = x_test[sel]\n",
    "    probs = model.predict(imgs, verbose=0)\n",
    "    preds = probs.argmax(1)\n",
    "    grid = np.zeros((2*32, 8*32, 3))\n",
    "    block = 32\n",
    "    for i in range(16):\n",
    "        r = i//8; c = i%8\n",
    "        im = denorm(imgs[i])\n",
    "        grid[r*block:(r+1)*block, c*block:(c+1)*block] = im\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.imshow(grid); plt.axis(\"off\")\n",
    "    title = \" | \".join(classes[p] for p in preds[:8])\n",
    "    plt.title(f\"Predictions: {title}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"artifacts/_tmp.png\", dpi=120); plt.close()\n",
    "    frames.append(imageio.imread(\"artifacts/_tmp.png\"))\n",
    "imageio.mimsave(\"artifacts/preds.gif\", frames, duration=0.9)\n",
    "\n",
    "# ========== 10) Rapport ==========\n",
    "print(classification_report(y_test, y_pred, target_names=classes))\n",
    "print(\"\\nSaved:\")\n",
    "print(\"- artifacts/confusion_matrix.png\")\n",
    "print(\"- artifacts/prediction_grid.png\")\n",
    "print(\"- artifacts/wrong_predictions_grid.png\")\n",
    "print(\"- artifacts/gradcam_grid.png\")\n",
    "print(\"- artifacts/preds.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc4080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# California Housing — XGBoost zonder callbacks/early_stopping, pure sklearn-API\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import uniform, randint\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "# 1) Data\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# 2) Train/Test split (testset pas op het eind gebruiken)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "# 3) Basismodel (geen callbacks, geen early_stopping, alles via sklearn-API)\n",
    "base = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    # Houd n_estimators redelijk; tuning kiest de beste waarde\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.1,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=0.8,\n",
    "    max_depth=5,\n",
    "    tree_method=\"hist\",   # \"gpu_hist\" + predictor=\"gpu_predictor\" als je GPU hebt\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    # eval_metric in constructor i.p.v. fit (compatibel met oudere wrappers)\n",
    "    eval_metric=\"rmse\"\n",
    ")\n",
    "\n",
    "# 4) Tuning-ruimte (zonder early stopping)\n",
    "param_dist = {\n",
    "    \"max_depth\": randint(3, 9),             # 3..8\n",
    "    \"learning_rate\": uniform(0.02, 0.18),   # 0.02..0.20\n",
    "    \"n_estimators\": randint(100, 1201),     # 100..1200\n",
    "    \"subsample\": uniform(0.7, 0.3),         # 0.70..1.00\n",
    "    \"colsample_bytree\": uniform(0.7, 0.3),  # 0.70..1.00\n",
    "    \"reg_lambda\": uniform(0.0, 2.0),        # L2-regularisatie\n",
    "    \"reg_alpha\": uniform(0.0, 0.5),         # L1-regularisatie\n",
    "}\n",
    "\n",
    "# 5) RandomizedSearchCV — optimaliseer op RMSE\n",
    "rs = RandomizedSearchCV(\n",
    "    estimator=base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=40,                              # pas aan naar wens (meer = beter)\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# 6) Fit (geen extra kwargs!)\n",
    "rs.fit(X_train, y_train)\n",
    "\n",
    "best_model = rs.best_estimator_\n",
    "print(\"Best params:\", rs.best_params_)\n",
    "\n",
    "# 7) Evaluatie op de testset\n",
    "y_pred = best_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"MAE : {mae:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"R^2 : {r2:.3f}\")\n",
    "\n",
    "# 8) (Optioneel) Feature importance (gain)\n",
    "importances = best_model.get_booster().get_score(importance_type=\"gain\")\n",
    "name_map = {f\"f{i}\": n for i, n in enumerate(feature_names)}\n",
    "importances_named = {name_map.get(k, k): v for k, v in importances.items()}\n",
    "print(\"\\nTop features (gain):\")\n",
    "for k in sorted(importances_named, key=importances_named.get, reverse=True)[:10]:\n",
    "    print(f\"- {k}: {importances_named[k]:.3f}\")\n",
    "\n",
    "# 9) (Optioneel) Model opslaan/laden\n",
    "# best_model.save_model(\"best_model_housing.ubj\")\n",
    "# loaded = XGBRegressor()\n",
    "# loaded.load_model(\"best_model_housing.ubj\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
